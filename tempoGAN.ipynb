{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ee8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path as osp\n",
    "\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from matplotlib import image as Img\n",
    "from PIL import Image\n",
    "from skimage.metrics import mean_squared_error\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import paddle\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import functions as func_module\n",
    "import hydra\n",
    "import numpy as np\n",
    "import paddle\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import ppsci\n",
    "from ppsci.utils import checker\n",
    "from ppsci.utils import logger\n",
    "from ppsci.utils import save_load\n",
    "\n",
    "import ppsci\n",
    "from ppsci.utils import checker\n",
    "from ppsci.utils import logger\n",
    "from ppsci.utils import save_load\n",
    "\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a7110",
   "metadata": {},
   "source": [
    "# functions about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def interpolate(\n",
    "    data: paddle.Tensor, ratio: int, mode: str = \"nearest\"\n",
    ") -> paddle.Tensor:\n",
    "    \"\"\"Interpolate twice.\n",
    "\n",
    "    Args:\n",
    "        data (paddle.Tensor): The data to be interpolated.\n",
    "        ratio (int): Ratio of one interpolation.\n",
    "        mode (str, optional): Interpolation method. Defaults to \"nearest\".\n",
    "\n",
    "    Returns:\n",
    "        paddle.Tensor: Data interpolated.\n",
    "    \"\"\"\n",
    "    for _ in range(2):\n",
    "        data = F.interpolate(\n",
    "            data,\n",
    "            [data.shape[-2] * ratio, data.shape[-1] * ratio],\n",
    "            mode=mode,\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "def reshape_input(input_dict: Dict[str, paddle.Tensor]) -> Dict[str, paddle.Tensor]:\n",
    "    \"\"\"Reshape input data for temporally Discriminator. Reshape data from N, C, W, H to N * C, 1, H, W.\n",
    "        Which will merge N dimension and C dimension to 1 dimension but still keep 4 dimensions\n",
    "        to ensure the data can be used for training.\n",
    "\n",
    "    Args:\n",
    "        input_dict (Dict[str, paddle.Tensor]): input data dict.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, paddle.Tensor]: reshaped data dict.\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for key in input_dict:\n",
    "        input = input_dict[key]\n",
    "        N, C, H, W = input.shape\n",
    "        out_dict[key] = paddle.reshape(input, [N * C, 1, H, W])\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def dereshape_input(\n",
    "    input_dict: Dict[str, paddle.Tensor], C: int\n",
    ") -> Dict[str, paddle.Tensor]:\n",
    "    \"\"\"Dereshape input data for temporally Discriminator. Deeshape data from 1, N * C, H, W to N, C, W, H.\n",
    "\n",
    "    Args:\n",
    "        input_dict (Dict[str, paddle.Tensor]): input data dict.\n",
    "        C (int): Channel of dereshape.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, paddle.Tensor]: dereshaped data dict.\n",
    "    \"\"\"\n",
    "    for key in input_dict:\n",
    "        input = input_dict[key]\n",
    "        _, N, H, W = input.shape\n",
    "        if N < C:\n",
    "            logger.warning(\n",
    "                f\"batch_size is smaller than {C}! Tempo needs at least {C} frames, input will be copied.\"\n",
    "            )\n",
    "            input_dict[key] = paddle.concat([input[:1]] * C, axis=1)\n",
    "        else:\n",
    "            N_new = int(N // C)\n",
    "            input_dict[key] = paddle.reshape(input[: N_new * C], [-1, C, H, W])\n",
    "    return input_dict\n",
    "\n",
    "\n",
    "# predict\n",
    "def split_data(data: np.ndarray, tile_ratio: int) -> np.ndarray:\n",
    "    \"\"\"Split a numpy image to tiles equally.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The image to be Split.\n",
    "        tile_ratio (int): How many tiles of one dim.\n",
    "            Number of result tiles is tile_ratio * tile_ratio for a 2d image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Tiles in [N,C,H,W] shape.\n",
    "    \"\"\"\n",
    "    _, _, h, w = data.shape\n",
    "    tile_h, tile_w = h // tile_ratio, w // tile_ratio\n",
    "    tiles = []\n",
    "    for i in range(tile_ratio):\n",
    "        for j in range(tile_ratio):\n",
    "            tiles.append(\n",
    "                data[\n",
    "                    :1,\n",
    "                    :,\n",
    "                    i * tile_h : i * tile_h + tile_h,\n",
    "                    j * tile_w : j * tile_w + tile_w,\n",
    "                ],\n",
    "            )\n",
    "    return np.concatenate(tiles, axis=0)\n",
    "\n",
    "\n",
    "def concat_data(data: np.ndarray, tile_ratio: int) -> np.ndarray:\n",
    "    \"\"\"Concat numpy tiles to a image equally.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The tiles to be upsplited.\n",
    "        tile_ratio (int): How many tiles of one dim.\n",
    "            Number of input tiles is tile_ratio * tile_ratio for 2d result.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image in [H,W] shape.\n",
    "    \"\"\"\n",
    "    _, _, tile_h, tile_w = data.shape\n",
    "    h, w = tile_h * tile_ratio, tile_w * tile_ratio\n",
    "    data_whole = np.ones([h, w], dtype=paddle.get_default_dtype())\n",
    "    tile_idx = 0\n",
    "    for i in range(tile_ratio):\n",
    "        for j in range(tile_ratio):\n",
    "            data_whole[\n",
    "                i * tile_h : i * tile_h + tile_h,\n",
    "                j * tile_w : j * tile_w + tile_w,\n",
    "            ] = data[tile_idx][0]\n",
    "            tile_idx += 1\n",
    "    return data_whole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2a523",
   "metadata": {},
   "source": [
    "# function predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d3ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_save_plot(\n",
    "    output_dir: str,\n",
    "    epoch_id: int,\n",
    "    solver_gen: ppsci.solver.Solver,\n",
    "    dataset_valid: np.ndarray,\n",
    "    tile_ratio: int = 1,\n",
    "):\n",
    "    \"\"\"Predicting and plotting.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Output dir path.\n",
    "        epoch_id (int): Which epoch it is.\n",
    "        solver_gen (ppsci.solver.Solver): Solver for predicting.\n",
    "        dataset_valid (np.ndarray): Valid dataset.\n",
    "        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n",
    "    \"\"\"\n",
    "    dir_pred = \"predict/\"\n",
    "    os.makedirs(os.path.join(output_dir, dir_pred), exist_ok=True)\n",
    "\n",
    "    start_idx = 190\n",
    "    density_low = dataset_valid[\"density_low\"][start_idx : start_idx + 3]\n",
    "    density_high = dataset_valid[\"density_high\"][start_idx : start_idx + 3]\n",
    "\n",
    "    # tile\n",
    "    density_low = (\n",
    "        split_data(density_low, tile_ratio) if tile_ratio != 1 else density_low\n",
    "    )\n",
    "    density_high = (\n",
    "        split_data(density_high, tile_ratio) if tile_ratio != 1 else density_high\n",
    "    )\n",
    "\n",
    "    pred_dict = solver_gen.predict(\n",
    "        {\n",
    "            \"density_low\": density_low,\n",
    "            \"density_high\": density_high,\n",
    "        },\n",
    "        {\"density_high\": lambda out: out[\"output_gen\"]},\n",
    "        batch_size=tile_ratio * tile_ratio if tile_ratio != 1 else 3,\n",
    "        no_grad=False,\n",
    "    )\n",
    "    if epoch_id == 1:\n",
    "        # plot interpolated input image\n",
    "        input_img = np.expand_dims(dataset_valid[\"density_low\"][start_idx], axis=0)\n",
    "        input_img = paddle.to_tensor(input_img, dtype=paddle.get_default_dtype())\n",
    "        input_img = F.interpolate(\n",
    "            input_img,\n",
    "            [input_img.shape[-2] * 4, input_img.shape[-1] * 4],\n",
    "            mode=\"nearest\",\n",
    "        ).numpy()\n",
    "        Img.imsave(\n",
    "            os.path.join(output_dir, dir_pred, \"input.png\"),\n",
    "            np.squeeze(input_img),\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            cmap=\"gray\",\n",
    "        )\n",
    "        # plot target image\n",
    "        Img.imsave(\n",
    "            os.path.join(output_dir, dir_pred, \"target.png\"),\n",
    "            np.squeeze(dataset_valid[\"density_high\"][start_idx]),\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            cmap=\"gray\",\n",
    "        )\n",
    "    # plot pred image\n",
    "    pred_img = (\n",
    "        concat_data(pred_dict[\"density_high\"].numpy(), tile_ratio)\n",
    "        if tile_ratio != 1\n",
    "        else np.squeeze(pred_dict[\"density_high\"][0].numpy())\n",
    "    )\n",
    "    Img.imsave(\n",
    "        os.path.join(output_dir, dir_pred, f\"pred_epoch_{str(epoch_id)}.png\"),\n",
    "        pred_img,\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"gray\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129640cf",
   "metadata": {},
   "source": [
    "# functions about eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8afdb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_img(\n",
    "    img_target: np.ndarray, img_pred: np.ndarray\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"Evaluate two images.\n",
    "\n",
    "    Args:\n",
    "        img_target (np.ndarray): Target image.\n",
    "        img_pred (np.ndarray): Image generated by prediction.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: MSE, PSNR, SSIM.\n",
    "    \"\"\"\n",
    "    eval_mse = mean_squared_error(img_target, img_pred)\n",
    "    eval_psnr = peak_signal_noise_ratio(img_target, img_pred)\n",
    "    eval_ssim = structural_similarity(img_target, img_pred, data_range=1.0)\n",
    "    return eval_mse, eval_psnr, eval_ssim\n",
    "\n",
    "\n",
    "def get_image_array(img_path):\n",
    "    return np.array(Image.open(img_path).convert(\"L\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a16742",
   "metadata": {},
   "source": [
    "# Generator, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35affdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenFuncs:\n",
    "    \"\"\"All functions used for Generator, including functions of transform and loss.\n",
    "\n",
    "    Args:\n",
    "        weight_gen (List[float]): Weights of L1 loss.\n",
    "        weight_gen_layer (List[float], optional): Weights of layers loss. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, weight_gen: List[float], weight_gen_layer: List[float] = None\n",
    "    ) -> None:\n",
    "        self.weight_gen = weight_gen\n",
    "        self.weight_gen_layer = weight_gen_layer\n",
    "\n",
    "    def transform_in(self, _in):\n",
    "        ratio = 2\n",
    "        input_dict = reshape_input(_in)\n",
    "        density_low = input_dict[\"density_low\"]\n",
    "        density_low_inp = interpolate(density_low, ratio, \"nearest\")\n",
    "        return {\"input_gen\": density_low_inp}\n",
    "\n",
    "    def loss_func_gen(self, output_dict: Dict, *args) -> paddle.Tensor:\n",
    "        \"\"\"Calculate loss of generator when use spatial discriminator.\n",
    "            The loss consists of l1 loss, l2 loss and layer loss when use spatial discriminator.\n",
    "            Notice that all item of loss is optional because weight of them might be 0.\n",
    "\n",
    "        Args:\n",
    "            output_dict (Dict): output dict of model.\n",
    "\n",
    "        Returns:\n",
    "            paddle.Tensor: Loss of generator.\n",
    "        \"\"\"\n",
    "        # l1 loss\n",
    "        loss_l1 = F.l1_loss(\n",
    "            output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n",
    "        )\n",
    "        losses = loss_l1 * self.weight_gen[0]\n",
    "\n",
    "        # l2 loss\n",
    "        loss_l2 = F.mse_loss(\n",
    "            output_dict[\"output_gen\"], output_dict[\"density_high\"], \"mean\"\n",
    "        )\n",
    "        losses += loss_l2 * self.weight_gen[1]\n",
    "\n",
    "        if self.weight_gen_layer is not None:\n",
    "            # disc(generator_out) loss\n",
    "            out_disc_from_gen = output_dict[\"out_disc_from_gen\"][-1]\n",
    "            label_ones = paddle.ones_like(out_disc_from_gen)\n",
    "            loss_gen = F.binary_cross_entropy_with_logits(\n",
    "                out_disc_from_gen, label_ones, reduction=\"mean\"\n",
    "            )\n",
    "            losses += loss_gen\n",
    "\n",
    "            # layer loss\n",
    "            key_list = list(output_dict.keys())\n",
    "            # [\"out0_layer0\",\"out0_layer1\",\"out0_layer2\",\"out0_layer3\",\"out_disc_from_target\",\n",
    "            # \"out1_layer0\",\"out1_layer1\",\"out1_layer2\",\"out1_layer3\",\"out_disc_from_gen\"]\n",
    "            loss_layer = 0\n",
    "            for i in range(1, len(self.weight_gen_layer)):\n",
    "                # i = 0,1,2,3\n",
    "                loss_layer += (\n",
    "                    self.weight_gen_layer[i]\n",
    "                    * F.mse_loss(\n",
    "                        output_dict[key_list[i]],\n",
    "                        output_dict[key_list[5 + i]],\n",
    "                        reduction=\"sum\",\n",
    "                    )\n",
    "                    / 2\n",
    "                )\n",
    "            losses += loss_layer * self.weight_gen_layer[0]\n",
    "\n",
    "        return {\"output_gen\": losses}\n",
    "\n",
    "    def loss_func_gen_tempo(self, output_dict: Dict, *args) -> paddle.Tensor:\n",
    "        \"\"\"Calculate loss of generator when use temporal discriminator.\n",
    "            The loss is cross entropy loss when use temporal discriminator.\n",
    "\n",
    "        Args:\n",
    "            output_dict (Dict): output dict of model.\n",
    "\n",
    "        Returns:\n",
    "            paddle.Tensor: Loss of generator.\n",
    "        \"\"\"\n",
    "        out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"][-1]\n",
    "        label_t_ones = paddle.ones_like(out_disc_tempo_from_gen)\n",
    "\n",
    "        loss_gen_t = F.binary_cross_entropy_with_logits(\n",
    "            out_disc_tempo_from_gen, label_t_ones, reduction=\"mean\"\n",
    "        )\n",
    "        losses = loss_gen_t * self.weight_gen[2]\n",
    "        return {\"out_disc_tempo_from_gen\": losses}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3b2a9",
   "metadata": {},
   "source": [
    "# Discriminator and temporally Discriminator !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19d1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscFuncs:\n",
    "    \"\"\"All functions used for Discriminator and temporally Discriminator, including functions of transform and loss.\n",
    "\n",
    "    Args:\n",
    "        weight_disc (float): Weight of loss generated by the discriminator to judge the true target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_disc: float) -> None:\n",
    "        self.weight_disc = weight_disc\n",
    "        self.model_gen = None\n",
    "\n",
    "    def transform_in(self, _in):\n",
    "        ratio = 2\n",
    "        input_dict = reshape_input(_in)\n",
    "        density_low = input_dict[\"density_low\"]\n",
    "        density_high_from_target = input_dict[\"density_high\"]\n",
    "\n",
    "        density_low_inp = interpolate(density_low, ratio, \"nearest\")\n",
    "\n",
    "        density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n",
    "        density_high_from_gen.stop_gradient = True\n",
    "\n",
    "        density_input_from_target = paddle.concat(\n",
    "            [density_low_inp, density_high_from_target], axis=1\n",
    "        )\n",
    "        density_input_from_gen = paddle.concat(\n",
    "            [density_low_inp, density_high_from_gen], axis=1\n",
    "        )\n",
    "        return {\n",
    "            \"input_disc_from_target\": density_input_from_target,\n",
    "            \"input_disc_from_gen\": density_input_from_gen,\n",
    "        }\n",
    "\n",
    "    def transform_in_tempo(self, _in):\n",
    "        density_high_from_target = _in[\"density_high\"]\n",
    "\n",
    "        input_dict = reshape_input(_in)\n",
    "        density_high_from_gen = self.model_gen(input_dict)[\"output_gen\"]\n",
    "        density_high_from_gen.stop_gradient = True\n",
    "\n",
    "        input_trans = {\n",
    "            \"input_tempo_disc_from_target\": density_high_from_target,\n",
    "            \"input_tempo_disc_from_gen\": density_high_from_gen,\n",
    "        }\n",
    "\n",
    "        return dereshape_input(input_trans, 3)\n",
    "\n",
    "    def loss_func(self, output_dict, *args):\n",
    "        out_disc_from_target = output_dict[\"out_disc_from_target\"]\n",
    "        out_disc_from_gen = output_dict[\"out_disc_from_gen\"]\n",
    "\n",
    "        label_ones = paddle.ones_like(out_disc_from_target)\n",
    "        label_zeros = paddle.zeros_like(out_disc_from_gen)\n",
    "\n",
    "        loss_disc_from_target = F.binary_cross_entropy_with_logits(\n",
    "            out_disc_from_target, label_ones, reduction=\"mean\"\n",
    "        )\n",
    "        loss_disc_from_gen = F.binary_cross_entropy_with_logits(\n",
    "            out_disc_from_gen, label_zeros, reduction=\"mean\"\n",
    "        )\n",
    "        losses = loss_disc_from_target * self.weight_disc + loss_disc_from_gen\n",
    "        return {\"CE_loss\": losses}\n",
    "\n",
    "    def loss_func_tempo(self, output_dict, *args):\n",
    "        out_disc_tempo_from_target = output_dict[\"out_disc_tempo_from_target\"]\n",
    "        out_disc_tempo_from_gen = output_dict[\"out_disc_tempo_from_gen\"]\n",
    "\n",
    "        label_ones = paddle.ones_like(out_disc_tempo_from_target)\n",
    "        label_zeros = paddle.zeros_like(out_disc_tempo_from_gen)\n",
    "\n",
    "        loss_disc_tempo_from_target = F.binary_cross_entropy_with_logits(\n",
    "            out_disc_tempo_from_target, label_ones, reduction=\"mean\"\n",
    "        )\n",
    "        loss_disc_tempo_from_gen = F.binary_cross_entropy_with_logits(\n",
    "            out_disc_tempo_from_gen, label_zeros, reduction=\"mean\"\n",
    "        )\n",
    "        losses = (\n",
    "            loss_disc_tempo_from_target * self.weight_disc + loss_disc_tempo_from_gen\n",
    "        )\n",
    "        return {\"CE_tempo_loss\": losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa14cd3",
   "metadata": {},
   "source": [
    "# transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe3f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFuncs:\n",
    "    \"\"\"All functions used for data transform.\n",
    "\n",
    "    Args:\n",
    "        tile_ratio (int, optional): How many tiles of one dim. Defaults to 1.\n",
    "        density_min (float, optional): Minimize density of one tile. Defaults to 0.02.\n",
    "        max_turn (int, optional): Maximize turn of taking a tile from one image. Defaults to 20.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tile_ratio: int = 1, density_min: float = 0.02, max_turn: int = 20\n",
    "    ) -> None:\n",
    "        self.tile_ratio = tile_ratio\n",
    "        self.density_min = density_min\n",
    "        self.max_turn = max_turn\n",
    "\n",
    "    def transform(\n",
    "        self,\n",
    "        input_item: Dict[str, np.ndarray],\n",
    "        label_item: Dict[str, np.ndarray],\n",
    "        weight_item: Dict[str, np.ndarray],\n",
    "    ) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "        if self.tile_ratio == 1:\n",
    "            return input_item, label_item, weight_item\n",
    "        for _ in range(self.max_turn):\n",
    "            rand_ratio = np.random.rand()\n",
    "            density_low = self.cut_data(input_item[\"density_low\"], rand_ratio)\n",
    "            density_high = self.cut_data(input_item[\"density_high\"], rand_ratio)\n",
    "            if self.is_valid_tile(density_low):\n",
    "                break\n",
    "\n",
    "        input_item[\"density_low\"] = density_low\n",
    "        input_item[\"density_high\"] = density_high\n",
    "        return input_item, label_item, weight_item\n",
    "\n",
    "    def cut_data(self, data: np.ndarray, rand_ratio: float) -> paddle.Tensor:\n",
    "        # data: C,H,W\n",
    "        _, H, W = data.shape\n",
    "        if H % self.tile_ratio != 0 or W % self.tile_ratio != 0:\n",
    "            exit(\n",
    "                f\"ERROR: input images cannot be divided into {self.tile_ratio} parts evenly!\"\n",
    "            )\n",
    "        tile_shape = [H // self.tile_ratio, W // self.tile_ratio]\n",
    "        rand_shape = np.floor(rand_ratio * (np.array([H, W]) - np.array(tile_shape)))\n",
    "        start = [int(rand_shape[0]), int(rand_shape[1])]\n",
    "        end = [int(rand_shape[0] + tile_shape[0]), int(rand_shape[1] + tile_shape[1])]\n",
    "        data = paddle.slice(\n",
    "            paddle.to_tensor(data), axes=[-2, -1], starts=start, ends=end\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def is_valid_tile(self, tile: paddle.Tensor):\n",
    "        img_density = tile[0].sum()\n",
    "        return img_density >= (\n",
    "            self.density_min * tile.shape[0] * tile.shape[1] * tile.shape[2]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32261a62",
   "metadata": {},
   "source": [
    "# tempo di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe372559",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d218425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg: DictConfig):\n",
    "    ppsci.utils.misc.set_random_seed(cfg.seed)\n",
    "    # initialize logger\n",
    "    logger.init_logger(\"ppsci\", osp.join(cfg.output_dir, \"train.log\"), \"info\")\n",
    "\n",
    "    gen_funcs = func_module.GenFuncs(\n",
    "        cfg.WEIGHT_GEN, (cfg.WEIGHT_GEN_LAYER if cfg.USE_SPATIALDISC else None)\n",
    "    )\n",
    "    disc_funcs = func_module.DiscFuncs(cfg.WEIGHT_DISC)\n",
    "    data_funcs = func_module.DataFuncs(cfg.TILE_RATIO)\n",
    "\n",
    "    # load dataset\n",
    "    logger.message(\n",
    "        \"Attention! Start loading datasets, this will take tens of seconds to several minutes, please wait patiently.\"\n",
    "    )\n",
    "    dataset_train = hdf5storage.loadmat(cfg.DATASET_PATH)\n",
    "    logger.message(\"Finish loading training dataset.\")\n",
    "    dataset_valid = hdf5storage.loadmat(cfg.DATASET_PATH_VALID)\n",
    "    logger.message(\"Finish loading validation dataset.\")\n",
    "\n",
    "    # define Generator model\n",
    "    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n",
    "    model_gen.register_input_transform(gen_funcs.transform_in)\n",
    "    disc_funcs.model_gen = model_gen\n",
    "\n",
    "    model_tuple = (model_gen,)\n",
    "    # define Discriminators\n",
    "    if cfg.USE_SPATIALDISC:\n",
    "        model_disc = ppsci.arch.Discriminator(**cfg.MODEL.disc_net)\n",
    "        model_disc.register_input_transform(disc_funcs.transform_in)\n",
    "        model_tuple += (model_disc,)\n",
    "\n",
    "    # define temporal Discriminators\n",
    "    if cfg.USE_TEMPODISC:\n",
    "        model_disc_tempo = ppsci.arch.Discriminator(**cfg.MODEL.tempo_net)\n",
    "        model_disc_tempo.register_input_transform(disc_funcs.transform_in_tempo)\n",
    "        model_tuple += (model_disc_tempo,)\n",
    "\n",
    "    # define model_list\n",
    "    model_list = ppsci.arch.ModelList(model_tuple)\n",
    "\n",
    "    # initialize Adam optimizer\n",
    "    lr_scheduler_gen = ppsci.optimizer.lr_scheduler.Step(\n",
    "        step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n",
    "    )()\n",
    "    optimizer_gen = ppsci.optimizer.Adam(lr_scheduler_gen)(model_gen)\n",
    "    if cfg.USE_SPATIALDISC:\n",
    "        lr_scheduler_disc = ppsci.optimizer.lr_scheduler.Step(\n",
    "            step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n",
    "        )()\n",
    "        optimizer_disc = ppsci.optimizer.Adam(lr_scheduler_disc)(model_disc)\n",
    "    if cfg.USE_TEMPODISC:\n",
    "        lr_scheduler_disc_tempo = ppsci.optimizer.lr_scheduler.Step(\n",
    "            step_size=cfg.TRAIN.epochs // 2, **cfg.TRAIN.lr_scheduler\n",
    "        )()\n",
    "        optimizer_disc_tempo = ppsci.optimizer.Adam(lr_scheduler_disc_tempo)(\n",
    "            (model_disc_tempo,)\n",
    "        )\n",
    "\n",
    "    # Generator\n",
    "    # manually build constraint(s)\n",
    "    sup_constraint_gen = ppsci.constraint.SupervisedConstraint(\n",
    "        {\n",
    "            \"dataset\": {\n",
    "                \"name\": \"NamedArrayDataset\",\n",
    "                \"input\": {\n",
    "                    \"density_low\": dataset_train[\"density_low\"],\n",
    "                    \"density_high\": dataset_train[\"density_high\"],\n",
    "                },\n",
    "                \"transforms\": (\n",
    "                    {\n",
    "                        \"FunctionalTransform\": {\n",
    "                            \"transform_func\": data_funcs.transform,\n",
    "                        },\n",
    "                    },\n",
    "                ),\n",
    "            },\n",
    "            \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n",
    "            \"sampler\": {\n",
    "                \"name\": \"BatchSampler\",\n",
    "                \"drop_last\": False,\n",
    "                \"shuffle\": False,\n",
    "            },\n",
    "        },\n",
    "        ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen),\n",
    "        {\n",
    "            \"output_gen\": lambda out: out[\"output_gen\"],\n",
    "            \"density_high\": lambda out: out[\"density_high\"],\n",
    "        },\n",
    "        name=\"sup_constraint_gen\",\n",
    "    )\n",
    "    constraint_gen = {sup_constraint_gen.name: sup_constraint_gen}\n",
    "    if cfg.USE_TEMPODISC:\n",
    "        sup_constraint_gen_tempo = ppsci.constraint.SupervisedConstraint(\n",
    "            {\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"NamedArrayDataset\",\n",
    "                    \"input\": {\n",
    "                        \"density_low\": dataset_train[\"density_low_tempo\"],\n",
    "                        \"density_high\": dataset_train[\"density_high_tempo\"],\n",
    "                    },\n",
    "                    \"transforms\": (\n",
    "                        {\n",
    "                            \"FunctionalTransform\": {\n",
    "                                \"transform_func\": data_funcs.transform,\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                },\n",
    "                \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n",
    "                \"sampler\": {\n",
    "                    \"name\": \"BatchSampler\",\n",
    "                    \"drop_last\": False,\n",
    "                    \"shuffle\": False,\n",
    "                },\n",
    "            },\n",
    "            ppsci.loss.FunctionalLoss(gen_funcs.loss_func_gen_tempo),\n",
    "            {\n",
    "                \"output_gen\": lambda out: out[\"output_gen\"],\n",
    "                \"density_high\": lambda out: out[\"density_high\"],\n",
    "            },\n",
    "            name=\"sup_constraint_gen_tempo\",\n",
    "        )\n",
    "        constraint_gen[sup_constraint_gen_tempo.name] = sup_constraint_gen_tempo\n",
    "\n",
    "    # Discriminators\n",
    "    # manually build constraint(s)\n",
    "    if cfg.USE_SPATIALDISC:\n",
    "        sup_constraint_disc = ppsci.constraint.SupervisedConstraint(\n",
    "            {\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"NamedArrayDataset\",\n",
    "                    \"input\": {\n",
    "                        \"density_low\": dataset_train[\"density_low\"],\n",
    "                        \"density_high\": dataset_train[\"density_high\"],\n",
    "                    },\n",
    "                    \"label\": {\n",
    "                        \"out_disc_from_target\": np.ones(\n",
    "                            (np.shape(dataset_train[\"density_high\"])[0], 1),\n",
    "                            dtype=paddle.get_default_dtype(),\n",
    "                        ),\n",
    "                        \"out_disc_from_gen\": np.ones(\n",
    "                            (np.shape(dataset_train[\"density_high\"])[0], 1),\n",
    "                            dtype=paddle.get_default_dtype(),\n",
    "                        ),\n",
    "                    },\n",
    "                    \"transforms\": (\n",
    "                        {\n",
    "                            \"FunctionalTransform\": {\n",
    "                                \"transform_func\": data_funcs.transform,\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                },\n",
    "                \"batch_size\": cfg.TRAIN.batch_size.sup_constraint,\n",
    "                \"sampler\": {\n",
    "                    \"name\": \"BatchSampler\",\n",
    "                    \"drop_last\": False,\n",
    "                    \"shuffle\": False,\n",
    "                },\n",
    "            },\n",
    "            ppsci.loss.FunctionalLoss(disc_funcs.loss_func),\n",
    "            name=\"sup_constraint_disc\",\n",
    "        )\n",
    "        constraint_disc = {sup_constraint_disc.name: sup_constraint_disc}\n",
    "\n",
    "    # temporal Discriminators\n",
    "    # manually build constraint(s)\n",
    "    if cfg.USE_TEMPODISC:\n",
    "        sup_constraint_disc_tempo = ppsci.constraint.SupervisedConstraint(\n",
    "            {\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"NamedArrayDataset\",\n",
    "                    \"input\": {\n",
    "                        \"density_low\": dataset_train[\"density_low_tempo\"],\n",
    "                        \"density_high\": dataset_train[\"density_high_tempo\"],\n",
    "                    },\n",
    "                    \"label\": {\n",
    "                        \"out_disc_tempo_from_target\": np.ones(\n",
    "                            (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n",
    "                            dtype=paddle.get_default_dtype(),\n",
    "                        ),\n",
    "                        \"out_disc_tempo_from_gen\": np.ones(\n",
    "                            (np.shape(dataset_train[\"density_high_tempo\"])[0], 1),\n",
    "                            dtype=paddle.get_default_dtype(),\n",
    "                        ),\n",
    "                    },\n",
    "                    \"transforms\": (\n",
    "                        {\n",
    "                            \"FunctionalTransform\": {\n",
    "                                \"transform_func\": data_funcs.transform,\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                },\n",
    "                \"batch_size\": int(cfg.TRAIN.batch_size.sup_constraint // 3),\n",
    "                \"sampler\": {\n",
    "                    \"name\": \"BatchSampler\",\n",
    "                    \"drop_last\": False,\n",
    "                    \"shuffle\": False,\n",
    "                },\n",
    "            },\n",
    "            ppsci.loss.FunctionalLoss(disc_funcs.loss_func_tempo),\n",
    "            name=\"sup_constraint_disc_tempo\",\n",
    "        )\n",
    "        constraint_disc_tempo = {\n",
    "            sup_constraint_disc_tempo.name: sup_constraint_disc_tempo\n",
    "        }\n",
    "\n",
    "    # initialize solver\n",
    "    solver_gen = ppsci.solver.Solver(\n",
    "        model_list,\n",
    "        constraint_gen,\n",
    "        cfg.output_dir,\n",
    "        optimizer_gen,\n",
    "        lr_scheduler_gen,\n",
    "        cfg.TRAIN.epochs_gen,\n",
    "        cfg.TRAIN.iters_per_epoch,\n",
    "        eval_during_train=cfg.TRAIN.eval_during_train,\n",
    "        use_amp=cfg.USE_AMP,\n",
    "        amp_level=cfg.TRAIN.amp_level,\n",
    "    )\n",
    "    if cfg.USE_SPATIALDISC:\n",
    "        solver_disc = ppsci.solver.Solver(\n",
    "            model_list,\n",
    "            constraint_disc,\n",
    "            cfg.output_dir,\n",
    "            optimizer_disc,\n",
    "            lr_scheduler_disc,\n",
    "            cfg.TRAIN.epochs_disc,\n",
    "            cfg.TRAIN.iters_per_epoch,\n",
    "            eval_during_train=cfg.TRAIN.eval_during_train,\n",
    "            use_amp=cfg.USE_AMP,\n",
    "            amp_level=cfg.TRAIN.amp_level,\n",
    "        )\n",
    "    if cfg.USE_TEMPODISC:\n",
    "        solver_disc_tempo = ppsci.solver.Solver(\n",
    "            model_list,\n",
    "            constraint_disc_tempo,\n",
    "            cfg.output_dir,\n",
    "            optimizer_disc_tempo,\n",
    "            lr_scheduler_disc_tempo,\n",
    "            cfg.TRAIN.epochs_disc_tempo,\n",
    "            cfg.TRAIN.iters_per_epoch,\n",
    "            eval_during_train=cfg.TRAIN.eval_during_train,\n",
    "            use_amp=cfg.USE_AMP,\n",
    "            amp_level=cfg.TRAIN.amp_level,\n",
    "        )\n",
    "\n",
    "    PRED_INTERVAL = 200\n",
    "    for i in range(1, cfg.TRAIN.epochs + 1):\n",
    "        logger.message(f\"\\nEpoch: {i}\\n\")\n",
    "        # plotting during training\n",
    "        if i == 1 or i % PRED_INTERVAL == 0 or i == cfg.TRAIN.epochs:\n",
    "            func_module.predict_and_save_plot(\n",
    "                cfg.output_dir, i, solver_gen, dataset_valid, cfg.TILE_RATIO\n",
    "            )\n",
    "\n",
    "        disc_funcs.model_gen = model_gen\n",
    "        # train disc, input: (x,y,G(x))\n",
    "        if cfg.USE_SPATIALDISC:\n",
    "            solver_disc.train()\n",
    "\n",
    "        # train disc tempo, input: (y_3,G(x)_3)\n",
    "        if cfg.USE_TEMPODISC:\n",
    "            solver_disc_tempo.train()\n",
    "\n",
    "        # train gen, input: (x,)\n",
    "        solver_gen.train()\n",
    "\n",
    "    ############### evaluation for training ###############\n",
    "    img_target = (\n",
    "        func_module.get_image_array(\n",
    "            os.path.join(cfg.output_dir, \"predict\", \"target.png\")\n",
    "        )\n",
    "        / 255.0\n",
    "    )\n",
    "    img_pred = (\n",
    "        func_module.get_image_array(\n",
    "            os.path.join(\n",
    "                cfg.output_dir, \"predict\", f\"pred_epoch_{cfg.TRAIN.epochs}.png\"\n",
    "            )\n",
    "        )\n",
    "        / 255.0\n",
    "    )\n",
    "    eval_mse, eval_psnr, eval_ssim = func_module.evaluate_img(img_target, img_pred)\n",
    "    logger.message(f\"MSE: {eval_mse}, PSNR: {eval_psnr}, SSIM: {eval_ssim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e11f0",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d1fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cfg: DictConfig):\n",
    "    from matplotlib import image as Img\n",
    "\n",
    "    from deploy.python_infer import pinn_predictor\n",
    "\n",
    "    # set model predictor\n",
    "    predictor = pinn_predictor.PINNPredictor(cfg)\n",
    "\n",
    "    # load dataset\n",
    "    dataset_infer = {\n",
    "        \"density_low\": hdf5storage.loadmat(cfg.DATASET_PATH_VALID)[\"density_low\"]\n",
    "    }\n",
    "\n",
    "    output_dict = predictor.predict(dataset_infer, cfg.INFER.batch_size)\n",
    "\n",
    "    # mapping data to cfg.INFER.output_keys\n",
    "    output = [output_dict[key] for key in output_dict]\n",
    "\n",
    "    def scale(data):\n",
    "        smax = np.max(data)\n",
    "        smin = np.min(data)\n",
    "        return (data - smin) / (smax - smin)\n",
    "\n",
    "    for i, img in enumerate(output[0]):\n",
    "        img = scale(np.squeeze(img))\n",
    "        Img.imsave(\n",
    "            osp.join(cfg.output_dir, f\"out_{i}.png\"),\n",
    "            img,\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            cmap=\"gray\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67e26b",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a8696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export(cfg: DictConfig):\n",
    "    from paddle.static import InputSpec\n",
    "\n",
    "    # set models\n",
    "    gen_funcs = func_module.GenFuncs(cfg.WEIGHT_GEN, None)\n",
    "    model_gen = ppsci.arch.Generator(**cfg.MODEL.gen_net)\n",
    "    model_gen.register_input_transform(gen_funcs.transform_in)\n",
    "\n",
    "    # define model_list\n",
    "    model_list = ppsci.arch.ModelList((model_gen,))\n",
    "\n",
    "    # load pretrained model\n",
    "    solver = ppsci.solver.Solver(\n",
    "        model=model_list, pretrained_model_path=cfg.INFER.pretrained_model_path\n",
    "    )\n",
    "\n",
    "    # export models\n",
    "    input_spec = [\n",
    "        {\"density_low\": InputSpec([None, 1, 128, 128], \"float32\", name=\"density_low\")},\n",
    "    ]\n",
    "    solver.export(input_spec, cfg.INFER.export_path, skip_prune_program=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e728d",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dcb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2025/06/17 08:34:52] ppsci MESSAGE: Attention! Start loading datasets, this will take tens of seconds to several minutes, please wait patiently.\u001b[0m\n",
      "\u001b[1;36m[2025/06/17 08:36:17] ppsci MESSAGE: Finish loading training dataset.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "path_cfg=\"./tempogan.yaml\"\n",
    "\n",
    "cfg = OmegaConf.load(path_cfg)\n",
    "# print(cfg.keys())\n",
    "# print(cfg.output_dir)\n",
    "train(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cda01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2025/06/17 08:37:32] ppsci MESSAGE: Inference with engine: native, precision: fp32, device: gpu.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Running PIR pass [add_shadow_output_after_dead_parameter_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [map_op_to_another_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [identity_op_clean_pass]\u001b[0m\n",
      "I0617 08:37:32.645927 299297 print_statistics.cc:50] --- detected [1] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [silu_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [conv2d_bn_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [conv2d_add_act_fuse_pass]\u001b[0m\n",
      "I0617 08:37:32.686297 299297 print_statistics.cc:50] --- detected [2] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [conv2d_add_fuse_pass]\u001b[0m\n",
      "I0617 08:37:32.687911 299297 print_statistics.cc:50] --- detected [10] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [embedding_eltwise_layernorm_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [fused_rotary_position_embedding_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [fused_flash_attn_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [multihead_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [matmul_add_act_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [fc_elementwise_layernorm_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [add_norm_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [group_norm_silu_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [matmul_scale_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [matmul_transpose_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [transpose_flatten_concat_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [remove_redundant_transpose_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [horizontal_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [transfer_layout_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [common_subexpression_elimination_pass]\u001b[0m\n",
      "I0617 08:37:32.694200 299297 print_statistics.cc:50] --- detected [24] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [params_sync_among_devices_pass]\u001b[0m\n",
      "I0617 08:37:32.704545 299297 print_statistics.cc:50] --- detected [60] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [constant_folding_pass]\u001b[0m\n",
      "I0617 08:37:32.708307 299297 pir_interpreter.cc:1619] New Executor is Running ...\n",
      "I0617 08:37:32.708847 299297 pir_interpreter.cc:1643] pir interpreter is running by multi-thread mode ...\n",
      "I0617 08:37:32.754418 299297 print_statistics.cc:44] --- detected [17, 120] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [dead_code_elimination_pass]\u001b[0m\n",
      "I0617 08:37:32.754770 299297 print_statistics.cc:50] --- detected [17] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass]\u001b[0m\n",
      "I0617 08:37:32.754912 299297 print_statistics.cc:50] --- detected [1] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [remove_shadow_feed_pass]\u001b[0m\n",
      "I0617 08:37:32.758000 299297 print_statistics.cc:50] --- detected [1] subgraphs!\n",
      "\u001b[1m\u001b[35m--- Running PIR pass [inplace_pass]\u001b[0m\n",
      "I0617 08:37:32.765772 299297 print_statistics.cc:50] --- detected [9] subgraphs!\n",
      "I0617 08:37:32.765885 299297 analysis_predictor.cc:1186] ======= pir optimization completed =======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/06/17 08:37:50] ppsci INFO: Predicting batch 1/800\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0617 08:37:50.666078 299297 pir_interpreter.cc:1640] pir interpreter is running by trace mode ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/06/17 08:37:52] ppsci INFO: Predicting batch 20/800\u001b[0m\n",
      "[2025/06/17 08:37:52] ppsci INFO: Predicting batch 40/800\u001b[0m\n",
      "[2025/06/17 08:37:53] ppsci INFO: Predicting batch 60/800\u001b[0m\n",
      "[2025/06/17 08:37:53] ppsci INFO: Predicting batch 80/800\u001b[0m\n",
      "[2025/06/17 08:37:53] ppsci INFO: Predicting batch 100/800\u001b[0m\n",
      "[2025/06/17 08:37:53] ppsci INFO: Predicting batch 120/800\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "path_cfg=\"./tempogan.yaml\"\n",
    "cfg = OmegaConf.load(path_cfg)\n",
    "\n",
    "inference(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f2ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2025/06/17 08:37:22] ppsci WARNING: Logger has already been automatically initialized as `log_file` is set to None by default, information will only be printed to terminal without writting to any file.\u001b[0m\n",
      "\u001b[1;36m[2025/06/17 08:37:22] ppsci MESSAGE: Found /root/.paddlesci/weights/tempogan_pretrained.pdparams already in /root/.paddlesci/weights, skip downloading.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0617 08:37:22.494938 299297 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.6, Runtime API Version: 12.6\n",
      "W0617 08:37:22.529381 299297 gpu_resources.cc:164] device: 0, cuDNN Version: 9.10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[2025/06/17 08:37:22] ppsci MESSAGE: Finish loading pretrained model from: /root/.paddlesci/weights/tempogan_pretrained.pdparams\u001b[0m\n",
      "[2025/06/17 08:37:22] ppsci INFO: Using paddlepaddle 3.0.0 on device Place(gpu:0)\u001b[0m\n",
      "\u001b[1;36m[2025/06/17 08:37:22] ppsci MESSAGE: Set to_static=False for computational optimization.\u001b[0m\n",
      "\u001b[1;36m[2025/06/17 08:37:24] ppsci MESSAGE: Inference model has been exported to: ./inference/tempoGAN, including *.json, *.pdiparams files.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle/lib/python3.9/site-packages/paddle/jit/api.py:662: UserWarning: Found 'dict' in given outputs, the values will be returned in a sequence sorted in lexicographical order by their keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "path_cfg=\"./tempogan.yaml\"\n",
    "cfg = OmegaConf.load(path_cfg)\n",
    "\n",
    "export(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
